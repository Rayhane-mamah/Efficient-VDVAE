[run]
# Run section: Defines the most important randomness and hardware params

# run.name: Mandatory argument, used to identify runs for save and restore
name = cifar10_baseline
# run.seed: seed that fixes all randomness in the project
seed = 420

# Hardware
# Global run config for GPUs and CPUs
num_gpus = 2
num_cpus = 256

# JAX only: Defines how many checkpoints will be kept on disk (the latest N)
max_allowed_checkpoints = 5


[data]
# Data section: Defines the dataset parameters
# To change a dataset to run the code on:
#   - Change the data.dataset_source to reflect which dataset you're trying to run.
#           This controls which data loading scripts to use and how to normalize
#   - Change the paths. For all datasets but binarized_mnist and cifar-10, define where the data lives on disk.
#   - Change the metadata: Define the image resolution, the number of channels and the color bit-depth of the data.

# Dataset source. Can be one of ('binarized_mnist', 'cifar-10', 'imagenet', 'celebA', 'celebAHQ', 'ffhq')
dataset_source = 'cifar-10'

# Data paths. Not used for (binarized_mnist, cifar-10)
train_data_path = '../datasets/imagenet_32/train_data/'
val_data_path = '../datasets/imagenet_32/val_data/'
synthesis_data_path = '../datasets/imagenet_32/val_data/'

# Image metadata
# Image resolution of the dataset (High and Width, assumed square)
target_res = 32
# Image channels of the dataset (Number of color channels)
channels = 3
# Image color depth in the dataset (bit-depth of each color channel)
num_bits = 8.
# Whether to do a random horizontal flip of images when loading the data (no applicable to MNIST)
random_horizontal_flip = True


[model]
# Model section: Defines the model design (architecture) hyper-parameters
# Some of these parameters will likely never be changed.

# General
# Main experimentation params
# Whether to apply a scaling by 1/sqrt(L) at the end of each residual bottleneck block (minimal effect on stability)
stable_init = True
# Whether or not to intialize the prior latent layer as zeros (no effect)
initialize_prior_weights_as_zero = False
# Whether to use 1x1 convs in the beginning and end of the residual bottleneck block (effective at reducing the memory load)
use_1x1_conv = True

# Latent layer distribution base can be in ('std', 'logstd'). Determines if the model should predict std (with softplus) or logstd (std is computed with exp(logstd)).
distribution_base = 'std'
# Similarly for output layer
output_distribution_base = 'std'
# Latent layer Gradient smoothing beta. ln(2) ~= 0.6931472. Setting this parameter to 1. disables gradient smoothing (not recommended)
gradient_smoothing_beta = 0.6931472
# Similarly for output layer
output_gradient_smoothing_beta = 0.6931472

################################# Layers' structure parameters ####################################
# In the bottom-up block, a skip connection is only taken once every n_blocks_per_res + 1 residual blocks
# That skip connection is linked to all top-down blocks of the matching resolution defined by backwards index in down_strides
# Example: up_strides = [x, a, x, x] and down_strides = [y, y, b, y] | the skip from a is linked to all blocks of b.

# Both downsampling and upsampling is done in tandem with residual blocks in a downsample-last upsample-first manner.
# i.e, both the downsample block and the upsample block are considered to belong to the higher resolution.
# Example: up_strides = [2, 2, 4] and up_n_blocks_per_res = [5, 9, 3] means there are in reality [6, 10, 4] blocks in their respective resolutions
# Similarly for down_strides = [4, 2, 2] and down_n_blocks_per_res = [3, 9, 5] means there are in reality [6, 10, 4] blocks.

# The residual blocks in up_n_blocks_per_res are always run BEFORE the residual block in up_strides.
# Inversely, the residual blocks in down_n_blocks_per_res are always run AFTER the residual block in down_strides.
# Downsampling of images happens in the residual block of up_strides. Upsampling of images happens in the residual block of down_strides.
# Skip connections between bottom-up and top-down blocks are taken once every time an up_strides block is called.

# down_strides must always be equal to the inverted up_strides. but any symmetry beyond that is not necessary (Appendix C.2 of the paper)
# To design a model that only links the bottom-up and top-down blocks once per resolution (like VDVAE), one should define only one up_stride layer
# per resolution and define as many up_n_blocks_per_res before each as they want. down_strides is the symmetrical counterpart to up_strides and down_n_blocks_per_res
# should contain as many block that come after an up_stride as one desires.
# Example: up_strides = [2, 2, 4, 1], up_n_blocks_per_res = [4, 5, 9, 3], down_strides = [1, 4, 2, 2], down_n_blocks_per_res = [4, 4, 11, 7] and a target_res=32
# means that bottom-up and top-down are linked once in resolution 32x32, once in 16x16, once in 8x8 and once in 2x2. Bottom-up and top-down networks however don't have
# the same number of layers.
# To design a model that links every bottom-up block to every top-down block (forces architectural symmetry), one should ensure that up_n_blocks_per_res and
# down_n_blocks_per_res are all zeros.
# Example: up_strides = [1, 1, 2, 1, 2, 1, 1, 4, 1], down_strides = [1, 4, 1, 1, 2, 1, 2, 1, 1], up_n_blocks_per_res = down_n_blocks_per_res = [0] * 9 and a target_res=32
# means that we design 3 layers in resolution 32x32, 2 layers in 16x16, 3 layers in 8x8 and 1 layer in 2x2. All these layers are connected between the bottom-up and top-down blocks.
# It's also possible to design any combination of the two above examples (partial symmetry).
# Example: up_strides = [2, 2, 1, 1, 4, 1], up_n_blocks_per_res = [4, 5, 0, 0, 0, 0], down_strides = [1, 4, 1, 1, 2, 2], down_n_blocks_per_res = [0, 0, 2, 3, 7, 4] and a target_res=32

# Bottom-up block
# Input conv
input_conv_filters = 384
input_kernel_size = (1, 1)

# Bottom-up blocks
# ALL LISTS MUST HAVE THE SAME LENGTH
# Defines the stride value of each block that will send a connection to top-down
up_strides = [1] * 11 + [2] + [1] * 6 + [2] + [1] * 6 + [2] + [1] * 3 + [4] + [1] * 3
# Defines the number of residual blocks prior to a connection with top-down
up_n_blocks_per_res =  [0] * 33
# Defines the number of residual bottleneck blocks per bottom-up block (usually kept at 1)
up_n_blocks = [1] * 33
# Defines the number of middle layers in the residual bottleneck block (doesn't account for the first and last layer of the res block)
up_n_layers = [2] * 33
# Defines the filter size of the model per bottom-up block. If up_stride != 1, then the filter size can be changed from that of the input, else it is ignored.
up_filters = [384] * 33
# Defines the bottleneck ratio inside the bottleneck res block
up_mid_filters_ratio = [0.25] * 33
# Defines the kernel size of the convs of the res block
up_kernel_size = [3] * 30 + [1] * 3
# Defines the filter size of the skip projection (projection of the activation sent to top-down).
up_skip_filters = [384] * 33

# Latent layers
# Whether to use the residual distribution from NVAE (no effect on stability, NLL or memory)
use_residual_distribution = False

# Top-down blocks
# ALL LISTS MUST HAVE THE SAME LENGTH
# Defines the strides value of each top-down block that will be the first to receive an activation from bottom-up
# must be symmetric (inverted) form of up_strides
down_strides = [1] * 3 + [4] + [1] * 3 + [2] + [1] * 6 + [2] + [1] * 6 + [2] + [1] * 11
# Defines the number of residual blocks after a first connection with bottom-up (number of res blocks that re-use the same bottom-up activation)
down_n_blocks_per_res = [0] * 3 + [0] + [0] * 3 + [0] + [0] * 6 + [0] + [4] + [0] * 5 + [0] + [10] + [0] * 10
# Defines the number of residual bottleneck blocks per top-down block (usually kept at 1)
down_n_blocks = [1] * 33
# Defines the number of middle layers in the residual bottleneck block (doesn't account for the first and last layer of the res block)
down_n_layers = [2] * 33
# Defines the filter size of the model per top-down block. If down_strides != 1, then the filter size can be changed from that of the input, else it is ignored.
down_filters = [384] * 33
# Defines the bottleneck ratio inside the bottleneck res block
down_mid_filters_ratio = [0.25] * 33
# Defines the kernel size of the convs of the res block
down_kernel_size = [1] * 3 + [3] * 30
# Defines the number of gaussian variates per top-down block
down_latent_variates = [32] * 33

# Output conv
output_kernel_size = (1, 1)
num_output_mixtures = 10
##################################################################################################


[loss]
# Loss section: parameters relevant for the loss function(s)

# Defines the minimum logscale of the MoL layer (exp(-250 = 0) so it's disabled). Look at section 6 of the paper.
min_mol_logscale = -250.

# ELBO beta warmup (from NVAE). Doesn't make much of an effect but it's safe to use it to avoid posterior collapses as NVAE suggests.
# lambda of variational prior loss
# schedule can be in ('None', 'Logistic', 'Linear')
variation_schedule = 'Linear'

# linear beta schedule
vae_beta_anneal_start = 21
vae_beta_anneal_steps = 5000
vae_beta_min = 1e-4

# logistic beta schedule
vae_beta_activation_steps = 10000
vae_beta_growth_rate = 1e-5

# Balancing the KL divergence between levels of the hierarchy using a gamma balancing (from NVAE).
# Doesn't make much of an effect but it's safe to use it to avoid posterior collapses as NVAE suggests.
# It's usually safe to set it to only work for as many steps as the beta anneal or double that.
# Gamma schedule of variational loss
use_gamma_schedule = True
gamma_max_steps = 10000
scaled_gamma = True

#L2 weight decay
use_weight_decay = True
l2_weight = 1e-2


[metrics]
# Metrics section: For parameters used in metric logging.

# Threshold used to mark latent groups as "active". Purely for debugging, shouldn't be taken seriously.
latent_active_threshold = 1e-4


[optimizer]
# Optimizer section: Defines the parameters relevant for the optimization procedure

# Optimizer can be one of ('Radam', 'Adam', 'Adamax').
# Adam and Radam should be avoided on datasets when the global norm is large!!
type = 'Adamax'
# Learning rate decay scheme can be one of ('cosine', 'constant', 'exponential', 'noam')
learning_rate_scheme = 'cosine'
# Defines the initial learning rate value
learning_rate = 1e-3

# noam/constant/cosine warmup (not much of an effect, done in VDVAE)
warmup_steps = 100.

# exponential or cosine
# Defines the number of steps over which the learning rate decays to the minimim value (after decay_start)
decay_steps = 750000
# Defines the update at which the learning rate starts to decay
decay_start = 50000
# Defines the minimum learning rate value
min_learning_rate = 2e-4

# exponential only
# Defines the decay rate of the exponential learning rate decay
decay_rate = 0.5

# Adam/Radam/Adamax parameters
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Gradient
# Gradient clip_norm value should be defined for nats/dim loss.
clip_gradient_norm = False
gradient_clip_norm_value = 300.

# Whether or not to use gradient skipping. This is usually unnecessary when using gradient smoothing but it doesn't hurt to keep it for safety.
gradient_skip = True
# Defines the threshold at which to skip the update. Also defined for nats/dim loss.
gradient_skip_threshold = 800.


[init]
# Init section: For parameters useful during the initialization step of the model

# Solely for model initialization
batch_size = 1

[train]
# Train section: Defines parameters only useful during training

# The total number of training updates
total_train_steps = 800000
# training batch size (global for all devices)
batch_size = 32

# Exponential Moving Average
ema_decay = 0.9999
# Whether to resume the model training from its EMA weights (highly experimental, not recommended)
resume_from_ema = False

# JAX only (controls how often to print the training metrics to terminal). Set to 1 for maximum verbosity, or higher for less frequent updates
logging_interval_in_steps = 1

# Defines how often to save a model checkpoint and logs (tensorboard) to disk.
checkpoint_and_eval_interval_in_steps = 10000


[val]
# Val section: Defines parameters only useful during validation

# Defines how many validation samples to validate on every time we're going to write to tensorboard
# Reduce this number of faster validation. Very small subsets can be non descriptive of the overall distribution
n_samples_for_validation = 5000
# validation batch size (global for all devices)
batch_size = 32 * 2


[synthesis]
# Synthesis section: Defines parameters only useful during synthesis/inference

# The synthesis mode can be one of ('reconstruction', 'generation', 'div_stats', 'encoding')
synthesis_mode = 'reconstruction'

# Whether or not to use the EMA weights for inference
load_ema_weights = True

# reconstruction/encoding mode
# Defines the quantile at which to prune the latent space (section 7).
# Example: variate_masks_quantile = 0.03 means only 3% of the posteriors that encode the most information will be preserved, all the others will be replaced with the prior.
# Encoding mode will always automatically prune the latent space using this argument, so it's a good idea to run masked reconstruction (read below) to find a suitable value of
# variate_masks_quantile before running encoding mode.
variate_masks_quantile = 0.03

# Reconstruction mode
# Whether to save the targets during reconstruction (for debugging)
save_target_in_reconstruction = False
# Whether to prune the posteriors to variate_masks_quantile. If set to True, the reconstruction is run with only variate_masks_quantile posteriors.
# All the other variates will be replaced with the prior. Used to compute the NLL at different % of prune posteriors, and to determine an appropriate variate_masks_quantile
# that doesn't hurt NLL.
mask_reconstruction = False

# div_stats mode
# Defines the ratio of the training data to compute the average KL per variate on (used for masked reconstruction and encoding).
# Set to 1. to use the full training dataset. But that' usually an overkill as 5%, 10% or 20% of the dataset tends to be representative enough.
div_stats_subset_ratio = 0.2

# generation_mode
# Number of generated batches per temperature from the temperature_settings list.
n_generation_batches = 1
# Temperatures for unconditional generation from the prior.
# We generate n_generation_batches for each element of the temperature_settings list. This is implemented so that many temperatures can be tested in the same run for speed.
# The temperature_settings elements can be one of:
#     - A float: Example 0.8. Defines the temperature used for all the latent variates of the model
#     - A tuple of 3: Example ('linear', 0.6, 0.9). Defines a linearly increasing temperature scheme from the deepest to shallowest top-down block. (different temperatures per latent
#                             group)
#     - A list of size len(down_strides): Each element of the list defines the temperature for their respective top-down blocks.
temperature_settings = [0.8, 0.85, 0.9, 1., ('linear', 0.7, 0.9), ('linear', 0.9, 0.7), ('linear', 0.8, 1.), ('linear', 1., 0.8), ('linear', 0.8, 0.9)]
# Temperature of the output layer (usually kept at 1. for extra realism)
output_temperature = 1.

# inference batch size (all modes)
# The inference batch size is global for all GPUs for JAX only. Pytorch does not support multi-GPU inference.
batch_size = 32